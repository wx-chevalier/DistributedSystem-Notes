# Introduction

介绍同步之前，首先我们来看下一个基于 12306 的典型的例子:

```
int total = get_total_from_tickets();
if ( total < 1) {
    not_found("No tickets!";);
    return FAILURE;
}
/*以上为步骤一*/

ticket tk = get_ticket_from_tickets();
--total;
/*以上为步骤二*/

split_ticket2user(tk);
/*步骤三*/
```

我们假设一次执行单元为其中一个步骤，但是这样的系统是不能使用的，因为它只能支持同时一个用户抢票。想象一下多个用户同时抢票的简单情形：系统中只剩一 张票了，用户 A 先开始抢票，整个过程是这样的：A 先看还有一张票，也就是步骤一。此时，用户 B 使用了抢票插件，如有神助的在 A 完成步骤一尚未进行步骤二的 时候完成了步骤一，这个时候无论 A 和 B 谁先执行步骤二都意味只有一个人能抢到票。

导致上述事例结果的根本原因在于数据没有及时更新，即数据同步不及时。多线程下的具体情形可能是一个运行中的线程随时可能在它正在使用临界区(放置临界资 源的区域)的时候被抢占，而新调度的线程紧接着进入该临界区，这个时候就会发生竞争。如果是在对称处理器多线程下，就算一个线程能完整的完成它的任务而不 被抢断，但多处理器真正并行使得多条线程可以同时修改临界区，这使得及时同步数据变得非常困难。采用同步可以确保每条线程看到的数据是一致的。尤其是一条 线程在修改某一个值而其他线程也需要读取或修改这个值的时候，使用同步就可以保证访问的数据有效。
总结来说，要解决同步问题，主要是两种方式，一是加锁，而是原子操作。

## Terminology

### Volatile

原意是易变的。在计算机领域意思相同，指由该关键字修饰的变量的值易变，因此具有可见性。可见性是指当一个线程对临界资源进行修改后，在其他线程中可以看 到发生的变化。为了实现这种可见性，处理器和编译器忽略对该关键字修饰变量的优化，也就是不对它进行重排，也不会缓存。但该变量的使用却是非常危险的，因 为它的行为总是违反我们的直觉。具体原因有以下几方面：
虽然编译器不去对它进行优化，并且阻止 volatile 变量之间的重排(如 C/C++和 Java)。但是，它们可能和非 volatile 变量一起被重排 序。在 C/C++和早期 Java 内存模型中确实是这么实现的。Java 在新的内存模型下，不仅 volatile 变量不能彼此重排序，而且 volatile 周围的普通字段的也不再能够随便的重排序了(和旧模型不同)，但是 C/C++的 volatile 却并未支持。还有一点容易让人误解的是它并不具有原子性这 也是最容易犯错的地方。最后一点也是最能让使用者谨慎使用的理由是某些编译器或者是陈旧的 Java 虚拟机对该关键字的支持不完整。也许使用 volatile 最安全的方式是严格限制到只是一个 boolean 值并且是在完全与周围变量或操作无关的场合。但不能放弃使用 volatile 的原因是在 如今的处理器架构下，内存模型强大到处理器对 volatile 的操作性能和非 volatile 相差无几。

### 自旋锁

Linux 内核中最常见的锁，作用是在多核处理器间同步数据。这里的自旋是忙等待的意思。如果一个线程(这里指的是内核线程)已经持有了一个自旋锁，而另 一条线程也想要获取该锁，它就不停地循环等待，或者叫做自旋等待直到锁可用。可以想象这种锁不能被某个线程长时间持有，这会导致其他线程一直自旋，消耗处 理器。所以，自旋锁使用范围很窄，只允许短期内加锁。其实还有一种方式就是让等待线程睡眠直到锁可用，这样就可以消除忙等待。很明显后者优于前者的实现， 但是却不适用于此。我们来详细分析。如果我们使用第二种方式，我们要做几步操作：把该等待线程换出、等到锁可用在换入，有两次上下文切换的代价。这个代价 和短时间内自旋(实现起来也简单)相比，后者更能适应实际情况的需要。还有一点需要注意，试图获取一个已经持有自旋锁的线程再去获取这个自旋锁或导致死 锁，但其他操作系统并非如此。
自旋锁与互斥锁有点类似，只是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是 否该自旋锁的保持者已经释放了锁，"自旋"一词就是因此而得名。其作用是为了解决某项资源的互斥使用。因为自旋锁不会引起调用者睡眠，所以自旋锁的效率远 高于互斥锁。虽然它的效率比互斥锁高，但是它也有些不足之处：
1、自旋锁一直占用 CPU，他在未获得锁的情况下，一直运行－－自旋，所以占用着 CPU，如果不能在很短的时 间内获得锁，这无疑会使 CPU 效率降低。
2、在用自旋锁时有可能造成死锁，当递归调用时有可能造成死锁，调用有些其他函数也可能造成死锁，如 copy_to_user()、copy_from_user()、kmalloc()等。

自旋锁比较适用于锁使用者保持锁时间比较短的情况。正是由于自旋锁使用者一般保持锁时间非常短，因此选择自旋而不是睡眠是非常必要的，自旋锁的效率远高于互斥锁。信号量和读写信号量适合于保持时间较长的情况，它们会导致调用者睡眠，因此只能在进程上下文使用，而自旋锁适合于保持时间非常短的情况，它可以在任何上下文使用。如果被保护的共享资源只在进程上下文访问，使用信号量保护该共享资源非常合适，如果对共享资源的访问时间非常短，自旋锁也可以。但是如果被保护的共享资源需要在中断上下文访问(包括底半部即中断处理句柄和顶半部即软中断)，就必须使用自旋锁。自旋锁保持期间是抢占失效的，而信号量和读写信号量保持期间是可以被抢占的。自旋锁只有在内核可抢占或 SMP(多处理器)的情况下才真正需要，在单 CPU 且不可抢占的内核下，自旋锁的所有操作都是空操作。另外格外注意一点：自旋锁不能递归使用。

### 互斥锁

即对互斥量进行分加锁，和自旋锁类似，唯一不同的是竞争不到锁的线程会回去睡会觉，等到锁可用再来竞争，第一个切入的线程加锁后，其他竞争失败者继续回去 睡觉直到再次接到通知、竞争。互斥锁算是目前并发系统中最常用的一种锁，POSIX、C++11、Java 等均支持。处理 POSIX 的加锁比较普通 外，C++和 Java 的加锁方式很有意思。C++中可以使用一种 AutoLock(常见于 chromium 等开源项目中)工作方式类似 auto_ptr 智 能指针，在 C++11 中官方将其标准化为 std::lock_guard 和 std::unique_lock。Java 中使用 synchronized 紧 跟同步代码块(也可修饰方法)的方式同步代码，非常灵活。这两种实现都巧妙的利用各自语言特性实现了非常优雅的加锁方式。当然除此之外他们也支持传统的类 似于 POSIX 的加锁模式。

### 读写锁

支持两种模式的锁，当采用写模式上锁时与互斥锁相同，是独占模式。但读模式上锁可以被多个读线程读取。即写时使用互斥锁，读时采用共享锁，故又叫共享-独 占锁。一种常见的错误认为数据只有在写入时才需要锁，事实是即使是读操作也需要锁保护，如果不这么做的话，读写锁的读模式便毫无意义。

### 重入

也叫做锁递归，就是获取一个已经获取的锁。不支持线程获取它已经获取且尚未解锁的方式叫做不可递归或不支持重入。带重入特性的锁在重入时会判断是否同一个 线程，如果是，则使持锁计数器+1(0 代表没有被线程获取，又或者是锁被释放)。C++11 中同时支持两种锁，递归锁 std::recursive_mutex 和非递归 std::mutex。Java 的两种互斥锁实现以及读写锁实现均支持重入。POSIX 使用一种叫做重 入函数的方法保证函数的线程安全，锁粒度是调用而非线程。

## 死锁

线程在执行过程中等待锁释放，如果存在多个线程相互等待已经被加锁的资源，就会造成死锁。大多数语言的锁实现都支持重入的一个重要原因是一个函数体内加锁 的代码段中经常会调用其他函数，而其他函数内部同样加了相同的锁，在不支持重入的情况下，执行线程总是要获取自己尚未释放的锁。也就是说该条线程试图获取 一个自己已经获取而尚未释放的锁。死锁就此产生。还有最经典的哲学家就餐问题。

## 线程饥饿

互斥锁中提到获取不到锁的线程回去睡眠等待下一次竞争锁，如果下一次仍然得不到，就继续睡眠，这种持续得不到锁的情况我们称之为饥饿。一个很有意思的例子是关于小米手机饥饿营销的。将小米手机比作竞争资源，抢手机的用户就是线程，每次开抢都抢不到的用户就是线程饥饿。和饥饿相对的是公平，操作系统调度程序负责这种公平，使用分片或 nice 或执行比等方式避免得不到调度的线程活活饿死。Java 默认采用非公平的互斥锁(synchronized 是强制的，Lock 是可选的。关于 Java 内置锁和 Lock 锁的公平性讨论参见：[Java 中的 ReentrantLock 和 synchronized 两种锁定机制的对比](http://my.eoe.cn/niunaixiaoshu/archive/5227.html))，但是公平锁因为要防止饥饿需要根据线程调度策略做调整，所以性能会受到影响，而且一般情况下某条线程饿死的情况鲜有发生(因为调度本来就是不公平的)，因此默认都是非公平的。

## CAS

中译名比较交换。目前有一种特殊的并发方式叫做无锁并发，通过上文的说明大家应该马上清楚要使用 CAS 达到正确同步须由处理其提供支持。有一个叫做 Lock_Free 的算法提出了一种方案：确保执行它的所有线程中至少有一个能够继续往下执行。实现这个算法的技术叫做比较并交换，即 CAS。CAS 使用 乐观技术来更新值，如果在另一条线程更新了这个值，CAS 可以检测到这个错误。现在大多数处理器架构(包括 IA32 和 Sparc)都支持比较并交换 CAS 的原子操作，X86 下对应的是 CMPXCHG 汇编指令。CAS 原语负责将某处内存地址的值(1 个字节)与一个期望值进行比较，如果相等，则将该内存地址处的值替换为新值，CAS 操作用一行 C 代码描述如下：
return *add == old_val ? (*add = new_val) ? false;
目前 windows 内置、Gcc 内置、C++11 定义头文件和 Java 通过 java.util.concurrent.atomic 包提供对 CAS 操作的支持。

# Memory Consistency Memory

> - [内存一致性模型](http://blog.chinaunix.net/uid-25909722-id-3016122.html)

# Lock(锁实现)

锁，和现实中的门把锁有相同也有不同。我们把线程看做人，屋子是临界区，里面放的是东西是临界资源。每次只允许一个人进入屋子，当某个人进入屋子后会把门 锁上，当另一个人想进入屋子的时候，只能在门口等待(这里等待的方式有两种：忙等待，即循环查看是否屋子的人出来了。还有一种是先睡一觉，等屋子里的人出 来叫醒自己)。直到屋子里的人解锁出来，这个时候在门口等待的人才可以进去。这里有一个关键点需要保证：锁必须是原子性操作实现，决不能中途打断，由处理 器原语支持。锁的意义在于将操作做为一个执行单元以一种原子方式执行而不被打断，多线程下也不会互相干扰。但是锁会影响性能，这是因为一个加锁的临界资源 在被访问前必须获取对应的锁，获取该锁的线程将以独占的方式访问临界区。如果此时有其他线程同时访问临界区，则会因为无法获取这个锁而阻塞，显然，在临界 区强行通过加锁使线程执行串行化是需要牺牲一定的性能的。

## Semaphore(信号量)

信号量又称为信号灯，它是用来协调不同进程间的数据对象的，而最主要的应用是共享内存方式的进程间通信。本质上，信号量是一个计数器，它用来记录对某个资源(如共享内存)的存取状况。一般说来，为了获得共享资源，进程需要执行下列操作:
　　 (1) 测试控制该资源的信号量。
　　 (2) 若此信号量的值为正，则允许进行使用该资源。进程将信号量减 1。
　　 (3) 若此信号量为 0，则该资源目前不可用，进程进入睡眠状态，直至信号量值大于 0，进程被唤醒，转入步骤(1)。
　　 (4) 当进程不再使用一个信号量控制的资源时，信号量值加 1。如果此时有进程正在睡眠等待此信号量，则唤醒此进程。

信号量与普通整型变量的区别：
① 信号量(semaphore)是非负整型变量，除了初始化之外，它只能通过两个标准原子操作：wait(semap) , signal(semap) ; 来进行访问；
② 操作也被成为 PV 原语(P 来源于 Dutch proberen"测试"，V 来源于 Dutch verhogen"增加")，而普通整型变量则可以在任何语句块中被访问；
信号量与互斥锁之间的区别：

1. 互斥量用于线程的互斥，信号线用于线程的同步。

这是互斥量和信号量的根本区别，也就是互斥和同步之间的区别。

互斥：是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。

同步：是指在互斥的基础上(大多数情况)，通过其它机制实现访问者对资源的有序访问。在大多数情况下，同步已经实现了互斥，特别是所有写入资源的情况必定是互斥的。少数情况是指可以允许多个访问者同时访问资源

Semaphore 是多线程编程时一个很重要的概念，概念本身并不复杂，但要做到正确使用却不容易。这里我们从面向对象的角度来理解这个概念。现实生活中的任何对象或实体，我们都可以用 class 来描述它。Semaphore 也不例外，如果用 class 定义应该是这样：

![](https://github.com/music4kid/music4kid.github.io/blob/master/images/class_semaphore.png?raw=true)

Semaphore 对象里包含一个 count 值和一个队列对象，另外有两个对外 public 的方法，wait()和 signal()，需要特别注意的事，count 值代表的资源数量是不能为负的。为了理解这些属性和方法，我们可以类比一个现实生活中的例子。大家去餐厅吃饭，假设这个餐厅有 10 个座位，有 20 个吃货随机出发去这个餐厅吃饭，那么对应关系是这样的：

- count = 10，10 个座位。
- queue，餐厅位置有限，为了避免混乱，餐厅肯定会吃货们排队。
- wait()，吃货到了餐厅找服务员要位置点餐，这个行为就是 wait。
- signal()，吃货吃完了买单离开位置，这个行为就是 signal。

这其实是一个信号量应用的典型场景，这里关键在于正确理解 wait 和 signal 发生时都有哪些细节步骤。用代码来描述大概是这样：

![](https://github.com/music4kid/music4kid.github.io/blob/master/images/semaphore_wait_signal.png?raw=true)

**wait**

具体到餐厅到例子，20 个人随机出发去餐厅吃饭，有 10 个人先到，然后挨个执行 wait。前 10 个人执行 wait 的时候是有位置的，所以 count>0，这 10 个人每人都消耗掉一个座位开始吃饭。到第 11 个人到了都时候，count ＝＝ 0，没有位置了，所以被 suspend，开始加入排队都队列等待。后续所有人都慢慢的到来，但和第 11 个人一样，都只能排队。

**signal**

过了一段时间之后，有个人吃好结账离开了餐厅。这时候如果没有人在排队，位置数量 count ＋＋，没有其它事情发生。但如果有人在排队，比如上面的情况，有 10 个人在等待位置，餐厅会把排在第一个的人安排到刚才空出来的位置，count 值没有变化，但队列的人少了一个。

**特别注意**

对于 wait 和 signal 还有两点需要特别注意。也是平时我们使用 semaphore 时比较容易产生 bug 的地方。

- wait 和 signal 都是原子操作。可以简单理解为上面代码里 wait(),signal()两个函数都是加锁的。这个特性其实让 semaphore 的行为变得更简单清晰。大家想象，如果到餐厅的 10 个人是同时到达的，但不是依次询问餐厅是否有位置，而是 10 张嘴同时说话，同时找餐厅要位置，显然情况会变得复杂不好处理。
- wait 或者 signal 调用的顺序是不确定的。上面的例子中每个人都是随机时间出发，到达餐厅的顺序也是随机的，并不一定先出发的就先到。同理每个人吃饭的时间长短也不一定，有人快有人慢，所以吃好离开餐厅的时间点也是随机的。这里每个人都代表一个线程，因为操作系统线程调度策略导致到底哪个线程先执行也是不确定的。

## Mutex(互斥量)

理解了 Semaphore，再看 Mutex 就很简单了。可以把 Mutex 理解成 count ＝＝ 1 的 Semaphore。在使用 Mutex 的场景下，永远都只允许有一个线程在占有资源，其它的线程都必须等待。建议大家按照 count ＝ 1 把上面 Semaphore 的例子再在脑子里过一遍，加深理解。

## Lock(锁)

上面说的 Semaphore 和 Mutex 都是操作系统层面的基础概念。但具体到某个平台的时候，平台会对这两个概念再做一次封装以方便使用。比如在 iOS 上就有 NSLock 这个类，提供 lock(),unlock()两个功能。但其实 Lock 在概念上和 Mutex 是一致的。当然平台既然做了封装就会提供额外的功能或者做一些额外的处理。这也是为什么在 iOS 里，NSLock 的性能会比 pthread_mutex 会差一些。iOS 里各种锁性能对比可以参考[这篇文章](http://perpendiculo.us/2009/09/synchronized-nslock-pthread-osspinlock-showdown-done-right/)。

## Condition(条件变量)

另一个遇到机会相对较少的概念是 Condition，Condition 理解起来很容易和上面几个概念混淆，但是只要和 Semaphore 对比下不同之处理解就很简单了。Condition 甚至可以理解成一种“特殊”的 Semaphore。特殊之处就在于它没有 count(资源数)。它也有 wait 和 signal 两种行为。用代码简单表示是这样的：

![](https://github.com/music4kid/music4kid.github.io/blob/master/images/class_condition.png?raw=true)

逻辑其实变更简单了，可以从事件的角度去看待 Condition。每次 condition 调用 wait 的时候，表示它想等待某个事件的发生(不管之前有没有发生过)，所以一定是加入到等待队列当中。调用 signal 的时候，表示这个事件发生了，如果有线程在队列里等待，则取出其中一个来执行，后面的继续等待后续事件。如果队列是空的，这个事件就丢了，当什么也没有发生过。所以这里的关键在于对 count(资源)和事件的理解。

## 可重入锁

锁作为并发共享数据，保证一致性的工具，在 JAVA 平台有多种实现(如 synchronized 和 ReentrantLock 等等 ) 。这些已经写好提供的锁为我们开发提供了便利，但是锁的具体性质以及类型却很少被提及。本系列文章将分析 JAVA 下常见的锁名称以及特性，为大家答疑解 惑。可重入锁，也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。
在 JAVA 环境下 ReentrantLock 和 synchronized 都是 可重入锁。

```
public class Test implements Runnable{
 public synchronized void get(){
  System.out.println(Thread.currentThread().getId());
 //在子方法里又进入了锁
  set();
 }
 public synchronized void set(){
  System.out.println(Thread.currentThread().getId());
 }
 @Override
 public void run() {
  get();
 }
 public static void main(String[] args) {
  Test ss=new Test();
  new Thread(ss).start();
  new Thread(ss).start();
  new Thread(ss).start();
 }
}


```

两个例子最后的结果都是正确的，即 同一个线程 id 被连续输出两次。
结果如下：

```
Threadid: 8
Threadid: 8
Threadid: 10
Threadid: 10
Threadid: 9
Threadid: 9
```

可重入锁最大的作用是避免死锁。
我们以自旋锁作为例子。

```
public class SpinLock {
 private AtomicReference<Thread> owner =new AtomicReference<>();
 public void lock(){
  Thread current = Thread.currentThread();
  while(!owner.compareAndSet(null, current)){
  }
 }
 public void unlock (){
  Thread current = Thread.currentThread();
  owner.compareAndSet(current, null);
 }
}

```

对于自旋锁来说：
1、若有同一线程两调用 lock() ，会导致第二次调用 lock 位置进行自旋，产生了死锁
说明这个锁并不是可重入的。(在 lock 函数内，应验证线程是否为已经获得锁的线程)
2、若 1 问题已经解决，当 unlock()第一次调用时，就已经将锁释放了。实际上不应释放锁。
(采用计数次进行统计)

```
public class SpinLock1 {
 private AtomicReference<Thread> owner =new AtomicReference<>();
 private int count =0;
 public void lock(){
  Thread current = Thread.currentThread();
  if(current==owner.get()) {
   count++;
   return ;
  }
  while(!owner.compareAndSet(null, current)){
  }
 }
 public void unlock (){
  Thread current = Thread.currentThread();
  if(current==owner.get()){
   if(count!=0){
    count--;
   }else{
    owner.compareAndSet(current, null);
   }
  }
 }
}
```

# Lock Memory Model(锁的内存模型)

在多处理器下，多个处理器共享主存。为了效率并不要求处理器将更新立即同步到主存上。处理器拥有自己的缓存以保存这些更新，并且定期与主存同步。这种需要 定期同步的方式是为了保证缓存一致性(Cache Coherence)。在缓存一致性许允许的范围内，多个处理器可以拥有同一个共享数据的不同状态。内存模型提供了一种保证：规定共享数据在不同线程间的 状态总是一致的。它的复杂性在于要协调处理器和编译器在与多线程程序执行时的性能与数据同步状态之间的平衡。处理器和编译器的工作是通过优化指令执行顺序 添加缓存来加快指令执行速度。内存模型采用一组屏障指令来保证存储的一致性，当然是在尽可能少的牺牲性能的前提下。具体的编译器和处理器加快指令执行的方 法是代码重排、指令重排以及缓存。内存模型利用处理器提供的一组指令来保护数据一致性，这种方式称为内存屏障(Memory Barriers)。

## 代码重排

代码重排是指编译器对用户代码进行优化以提高代码的执行效率，优化前提是不改变代码的结果，即优化前后代码执行结果必须相同。譬如：

```
int a = 1, b = 2, c = 3;
void test() {
    a = b + 1;
    b = c + 1;
    c = a + b;
}
```

在 gcc 下的汇编代码 test 函数体代码如下：
编译参数: -O0

```

movl b(%rip), %eax
addl $1, %eax
movl %eax, a(%rip)
movl c(%rip), %eax
addl $1, %eax
movl %eax, b(%rip)
movl a(%rip), %edx
movl b(%rip), %eax
addl %edx, %eax
movl %eax, c(%rip)
```

编译参数：-O3

```

movl b(%rip), %eax                  ;将b读入eax寄存器
leal 1(%rax), %edx                  ;将b+1写入edx寄存器
movl c(%rip), %eax                  ;将c读入eax
movl %edx, a(%rip)                  ;将edx写入a
addl $1, %eax                       ;将eax+1
movl %eax, b(%rip)                  ;将eax写入b
addl %edx, %eax                     ;将eax+edx
movl %eax, c(%rip)                  ;将eax写入c
```

我在-O3 的汇编下做了详细的注释，参照注释和原 C 代码理解这两段汇编代码应该不难。当然编译器优化并没有做多少工作，这是因为并未有多少无用代码。但是如果我们的 test 函数体内只写了 100 行的 a++; 那汇编指令使用-O1 就会优化这 100 行代码成一条：

```
addl $100, a(%rip)
```

然而，上面的代码更有意义来说明编译器优化并且其中将后面可能用到的汇编指令做了清楚的注释以说明其含意，便于下文对汇编代码的理解。如果觉得上述代码的指令重排难于理解或是不够充分，接下来看这段 C 代码和 gcc -O3 下的汇编代码：

```

int a = 1, b = 2;
void test1() {
    a = b + 1;
    b = 39;
}
------
movl b(%rip), %eax ;读取b给eax
movl $39, b(%rip) ;向b写入39
addl $1, %eax ;eax+1
movl %eax, a(%rip) ;将eax写回a
```

很显然 b 先被赋值，然后才是 a。也就是说在该函数中，代码的执行顺序发生了变化，但却不影响最终结果。编译器重排是根据指令之间是否有数据依赖关系来决定 的，虽然看似 C 代码间存在依赖，但是重排却是指令级别的。顺便分析下这里为什么会把 b 的赋值操作提前进行呢？寄存器读入 b 的值时对 b 进行缓存，再写入的时 候直接从寄存器缓存中取出赋值避免了再次从高速缓存甚至主存中取出 b 再赋值。可以动手写实验代码验证，很容易发现确实编译器会将相关变量的操作提取到一起 执行，这是因为处理器充分利用寄存器缓存来加速指令执行。

## 指令重排

大多数主流的处理器为了效率可以调整读写操作的顺序，但为什么这么做呢？处理器在执行指令期间，会把指令按照自适应处理的最优情形进行重新排序，使指令执 行时间变得更短(绝大多数情形下，前提是不改变程序的执行结果)。处理器的具体做法是优化其指令流水线(Instruction pipeline)以减少指令执行时间。
我们假设在一条简单的流水线中，完成一个指令可能需要 5 层。对于一些并不互相依赖的指令，要在最佳性能下运算，当第一个指令被运行时，这个流水线需要运行 紧接着的 4 条独立的指令。如果有指令依赖前面已经执行的指令，那处理器就会通过某种方式延缓该条指令执行直到依赖的指令执行完毕。使用多个层执行指令，处 理器可以显著提高性能从而减少指令运行所需要的周期。处理器使用这种优化 Pipeline 的方式一方面提高了指令执行效率，但另一方面却出现了另一个麻 烦。单处理器下执行指令调整顺序在多线程并发的时候出现了困难。我们假设有两个处理器，每一个处理器执行一条线程，对于涉及到同一段代码对非局部变量赋值 的顺序会因为的每一个处理器对各自指令顺序调整而变得混乱。
看下启动服务器的示例代码：
全局：bool enable = false;

```

void start_server() {
    open_server();
    enable = true;
}

void http_server() {
    if(enable) handle_request();
}
```

我们使用两条线程在多处理器下并发，一条线程 A 负责启动服务器 start_server(),另一条线程 B 负责处理 http 请求。由于上述所说的处理器会 将互不依赖的两条指令调换顺序(这里暂且忽略编译器优化)，我们有理由相信会出现这样的情形：A 线程：enable = true;已经执行，但 open_server();还未调用。此时另一条线程 B 执行 http_server();发现服务可用，直接使用未打开的服务器 来处理请求。
同样的我们如果在此忽略处理器重排，只使用编译器的代码重排也会导致上述问题。问题的复杂性在于编译器和处理器同时作用下，指令的执行顺序更加神秘莫测。 但更糟糕的是还有一种为了效率缓存指令执行结果使数据不能及时更新的因素影响数据同步，这个因素就是 CPU Cache。

## CPU Cache

A trip to main memory costs hundreds of clock cycles on commodity hardware. Processors use caching to decrease the costs of memory latency by orders of magnitude.
——Dennis Byrne
上面这句话的大体意思是说对内存的一次访问需要花费数百个时钟周期，处理器使用缓存减少内存延迟的成本。这就是引入缓存的原因。CPU 内部结构根据物理距 离依次是：处理器、寄存器、CPU Cache。我根据自己机器上的 CPU Cache 的信息用 cacoo 画了一个简单版本的 CPU 内部结构图：
![](http://img.blog.csdn.net/20140128171814125?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYWppYW4wMDU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

![](http://mmbiz.qpic.cn/mmbiz/nfxUjuI2HXh2dBjY4yO4xib5qK2cfEDZwIFicOL0VADibmMib3kZSCibkZ15ricFD3qXicJicwXOKibw7ecBjicq7kJpzCbA/640?wx_fmt=png&wxfrom=5&wx_lazy=1)

参照上图，由于既要满足存储容量需求又要满足 CPU 的高速存取需求，目前的缓存被设计为多级 L1, L2 。。。。。。Ln。每一级的访问周期比上一级多一个数量级(并不绝对)。L1 靠近处理器，用以满足高速存取的需求，Ln 靠近主存，容积随之变大。目前主流 是三级缓存。Linux 下查看 CPU Cache 可以进入/sys/devices/system/cpu/cpu0/cache，例如我的机器下查看高速缓存类型的结果：

```
lunatic@ubuntu:/sys/devices/system/cpu/cpu0/cache$ cat -n index0/type index1/type index2/type index3/type

1 Data //L1, Data Cache
2 Instruction //L1, instruction Cache
3 Unified //L2，代表不共享
4 Unified //L3，代表不共享
```

处理器缓存执行指令结果于自己的 cache 上，等满足一定条件如遇到请求刷新的指令，再将缓存结果一并输出。
用一段挂起服务器的代码就可以看出缓存对数据同步的影响：
全局：Bool enable = true;

```

void halt_server() {
    enable = false;
    close_server();
}
void http_server() {
    if(enable) handle_request();
}
```

还是双核双线程模拟执行的情形：线程 A 执行服务器挂起操作，但因为 CPU 缓存影响，服务器已经挂起，但并未同步更新到全局 enable。而另一条线程 B 检测到 enable 可用时，就继续提供请求处理服务导致出现一个已经关闭的服务器还提供服务的情形。
对于处理器使用高速缓存以追求效率的解释可以通过对 C 语言标准库的 IO 缓冲分析来解释。C 语言的 IO 输出默认有三种缓冲方式：无缓冲、行缓冲和块缓冲。我 们以 printf(默认行缓冲)说明，printf 函数的作用是将格式化后的字符逐个发送到输出缓冲区，缓冲区被刷新直到遇到换行符。这样做无疑能提高效 率，并且延迟了写输出。再来看输入 scanf，从键盘缓冲区读取非空白字符数据，而把换行符’\n’遗留在缓冲区，引发的问题是如果同时用 getchar、gets 等不会忽略’\n’的函数读取，就只能读取到无意义的’\n’。C 语言 IO 系统的读入写出和处理器缓存非常类似。因此，解决 C 语 言缓冲区导致数据不一致的方案可以帮助理解处理器的缓存处理。C 语言使用 fflush 函数或其他类似功能函数在必要的时候刷新或者清空缓冲区。处理器在缓 存处理上采取类似的做法，但远比其复杂的是处理器要同时兼顾指令缓存以及指令排序还有其他影响到数据同步的各方面软硬件因素，这种多方兼顾的方式就叫做内存屏障。

## 内存屏障

现在我们来谈下多处理器下的共享内存数据同步问题。多处理器同时访问共享主存，每个处理器都要对读写进行重新排序，一旦数据更新，就需要同步更新到主存上 (这里并不要求处理器缓存更新之后立刻更新主存)。在这种情况下，代码和指令重排，再加上缓存延迟指令结果输出导致共享变量被修改的顺序发生了变化，使得 程序的行为变得无法预测。为了解决这种不可预测的行为，处理器提供一组机器指令来确保指令的顺序要求，它告诉处理器在继续执行前提交所有尚未处理的载入和 存储指令。同样的也可以要求编译器不要对给定点以及周围指令序列进行重排。这些确保顺序的指令称为内存屏障。具体的确保措施在程序语言级别的体现就是内存 模型的定义。POSIX、C++、Java 都有各自的共享内存模型，实现上并没有什么差异，只是在一些细节上稍有不同。这里所说的内存模型并非是指内存布 局，特指内存、Cache、CPU、写缓冲区、寄存器以及其他的硬件和编译器优化的交互时对读写指令操作提供保护手段以确保读写序。将这些繁杂因素可以笼 统的归纳为两个方面：重排和缓存，即上文所说的代码重排、指令重排和 CPU Cache。简单的说内存屏障做了两件事情：**拒绝重排，更新缓存**。
C++11 提供一组用户 API std::memory_order 来指导处理器读写顺序。Java 使用 happens-before 规则来屏蔽具体细节保证，指导 JVM 在指令生成的过程中穿插屏障指令。
内存屏障也可以在编译期间指示对指令或者包括周围指令序列不进行优化，称之为编译器屏障，相当于轻量级内存屏障，它的工作同样重要，因为它在编译期指导编译器优化。屏障的实现稍微复杂一些，我们使用一组抽象的假想指令来描述内存屏障的工作原理。
使用 MB_R、MB_W、MB 来抽象处理器指令为宏：
MB_R 代表读内存屏障，它保证读取操作不会重排到该指令调用之后。
MB_W 代表写内存屏障，它保证写入操作不会重排到该指令调用之后。
MB 代表读写内存屏障，可保证之前的指令不会重排到该指令调用之后。
这里用一个例子描述在多线程并发中使用内存屏障的意义：
全局变量：bool has = false, Ticket t = NULL;

```

Thread A                                                                Thread B
has = has_remain_ticket();/*有票*/             -------------------
mb();/*表示在该处插入MB指令 */               if(has) {;
--------------- --------------- --------------- ------  t = get_a_ticket();
--------------- --------------- --------------- ------  to_user(t);
--------------- --------------- --------------- -------------- }
```

用两条线程 A、B 执行，A 中加入的内存屏障有票，再通知用户，不会发生先通知用户，再去看有票。B 中的内存屏障保证必须取到票再交给用户，防止尚未取票，就交给用户。
这些屏障指令在单核处理器上同样有效，因为单处理器虽不涉及多处理器间数据同步问题，但指令重排和缓存仍然影响数据的正确同步。指令重排是非常底层的且实 现效果差异非常大，尤其是不同体系架构对内存屏障的支持程度，甚至在不支持指令重排的体系架构中根本不必使用屏障指令。具体如何使用这些屏障指令是支持的 平台、编译器或虚拟机要实现的，我们只需要使用这些实现的 API(指的是各种并发关键字、锁、以及重入性等，下节详细介绍)。这里的目的只是为了帮助更好 的理解内存屏障的工作原理。
内存屏障的意义重大，是确保正确并发的关键。通过正确的设置内存屏障可以确保指令按照我们期望的顺序执行。这里需要注意的是内存屏蔽只应该作用于需要同步的指令或者还可以包含周围指令的片段。如果用来同步所有指令，目前绝大多数处理器架构的设计就会毫无意义。

# Lock Strategy(锁策略)
