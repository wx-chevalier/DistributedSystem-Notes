# 反爬虫与绕过对抗

毫无疑问，爬虫破坏了网站的正常访问，造成了系统的额外负载。而反爬虫还能够保护数据，放置恶意的商业竞争。

# IP 限流与代理

当网站发现某个 IP 访问的速度不仅飞快，而且都是在同一个 IP 段的时候，会认为该 IP 在进行爬虫操作，因此会对该 IP 进行限制访问，拒绝传输数据。

解决方法： 首先通过设置 User-Agent 来模拟正常的访问流程，同时通过获取代理 IP 来爬取数据。

# 权限认证

必须登录才能访问。
我们在之前的文章中，通过设置 User-Agent 模拟浏览器，让服务器认为我们是一个正常的访问过程。当网站发现服务器压力非常大，而且都是正常的 User-Agent 时，往往会对部分比价重要的数据，设置为必须登录才能访问。

解决方法： 我们可以首先注册一个账号，带上账号的 Cookie 爬取数据。

保存 cookies，记录用户的状态，在模拟登陆十分麻烦的情况下，我们不妨直接在 web 上登陆之后取下 cookie 并保存然后带上 cookie 做爬虫，但这不是长久的方法，而且 cookie 隔一段时间可能失效。有的网站会根据 cookie 中的一些值去判断是否机器人，这个需要自己不断测试，比如豆瓣。

注意配合移动端、web 端以及桌面版，其中 web 端包括 m 站即手机站和 pc 站，往往是 pc 站的模拟抓取难度大于手机站，所以在 m 站和 pc 站的资源相同的情况下优先考虑抓取 m 站。同时如果无法在 web 端抓取，不可忽略在 app 以及桌面版的也可以抓取到目标数据资源。

# 验证码

滑动滑块。
这是目前比较流行的验证方法。即只有用户通过鼠标，将一部分的图像移动到对应的区域，才能够正常访问
解决方法： 目前的解决方法，大部分是首先对图像进行处理，然后模拟人工滑动的方式。（难）

对于处理验证码，爬虫爬久了通常网站的处理策略就是让你输入验证码验证是否机器人，此时有三种解决方法：

第一种把验证码 down 到本地之后，手动输入验证码验证，此种成本相对较高，而且不能完全做到自动抓取，需要人为干预。第二种图像识别验证码，自动填写验证，但是现在的情况是大部分验证码噪声较多复杂度大，对于像我这样对图像识别不是很熟悉的人很难识别出正确的验证码。第三种也是最实用的一种，接入自动打码平台，个人感觉比上两种方法好些。 2、多账号反爬，有很多的网站会通过同一个用户单位时间内操作频次来判断是否机器人，比如像新浪微博等网站。这种情况下我们就需要先测试单用户抓取阈值，然后在阈值前切换账号其他用户，如此循环即可。当然，新浪微博反爬手段不止是账号，还包括单 ip 操作频次等。

使用验证码验证登录。
网站使用验证码进行登录验证，查看网站必须在登录界面输入相应的验证码才能够访问。
解决方法： 使用机器学习算法，对验证码进行训练。或者是通过打码平台以及人工打码的方式进行处理。

# 数据编码

使用异步加载的方式加载数据。
网站的数据并不全部加载，只有当用户查看到最底部时，网站才会将剩下的数据加载出来。
解决方法： 正如我们之前学到的一样，AJAX 技术的确难以处理。我们需要通过不断地请求，去分析其规律，根据规律模拟出 AJAX 所请求的网页链接。

重要数据图像化。
网站在传输数据之前，将数据编码为图片的格式，通过图片来展示认为重要的数据。
解决方式： 使用 OCR 识别技术，识别图像中的数据。（难）

# 行为识别

行为识别。
目前大型的网站，例如阿里巴巴等，会对用户在网站上操作的每一个动作进行判断。当用户的动作具有间歇性、鼠标停留时间等特征与正常访问时的数据产生误差时，系统会判定为爬虫程序，并且拒绝相应的请求。

发送错误信息。
当网站识别出发出的请求是来自于爬虫程序时，并不会拒绝连接，而是为该请求提供一个错误的信息或者数据，一方面让爬虫程序误认为爬取到了想要的数据，另一方面，也降低了服务器的请求压力。
解决方法： 抽取 50%或者以上的数据进行抽查或者再次爬取。
